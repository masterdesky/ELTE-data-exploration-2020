{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axistitlesize = 28\n",
    "axisticksize = 16\n",
    "axislabelsize = 24\n",
    "axislegendsize = 20\n",
    "\n",
    "# Set axtick dimensions\n",
    "major_size = 6\n",
    "major_width = 1.2\n",
    "minor_size = 3\n",
    "minor_width = 1\n",
    "mpl.rcParams['xtick.major.size'] = major_size\n",
    "mpl.rcParams['xtick.major.width'] = major_width\n",
    "mpl.rcParams['xtick.minor.size'] = minor_size\n",
    "mpl.rcParams['xtick.minor.width'] = minor_width\n",
    "mpl.rcParams['ytick.major.size'] = major_size\n",
    "mpl.rcParams['ytick.major.width'] = major_width\n",
    "mpl.rcParams['ytick.minor.size'] = minor_size\n",
    "mpl.rcParams['ytick.minor.width'] = minor_width\n",
    "\n",
    "mpl.rcParams.update({'figure.autolayout': False})\n",
    "\n",
    "# Seaborn style settings\n",
    "sns.set_style({'axes.axisbelow': True,\n",
    "               'axes.edgecolor': '.8',\n",
    "               'axes.facecolor': 'white',\n",
    "               'axes.grid': True,\n",
    "               'axes.labelcolor': '.15',\n",
    "               'axes.spines.bottom': True,\n",
    "               'axes.spines.left': True,\n",
    "               'axes.spines.right': True,\n",
    "               'axes.spines.top': True,\n",
    "               'figure.facecolor': 'white',\n",
    "               'font.family': ['sans-serif'],\n",
    "               'font.sans-serif': ['Arial',\n",
    "                'DejaVu Sans',\n",
    "                'Liberation Sans',\n",
    "                'Bitstream Vera Sans',\n",
    "                'sans-serif'],\n",
    "               'grid.color': '.8',\n",
    "               'grid.linestyle': '--',\n",
    "               'image.cmap': 'rocket',\n",
    "               'lines.solid_capstyle': 'round',\n",
    "               'patch.edgecolor': 'w',\n",
    "               'patch.force_edgecolor': True,\n",
    "               'text.color': '.15',\n",
    "               'xtick.bottom': True,\n",
    "               'xtick.color': '.15',\n",
    "               'xtick.direction': 'in',\n",
    "               'xtick.top': True,\n",
    "               'ytick.color': '.15',\n",
    "               'ytick.direction': 'in',\n",
    "               'ytick.left': True,\n",
    "               'ytick.right': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Read in a bunch of tweets that were collected during the airing of the 'Red Wedding' episode of Game of Thrones from the file `got_tweets.csv`.\n",
    "\n",
    "Parse all the hashtags out of the texts, then count their overall occurrences. What are the 10 most common hashtags? Make a bar chart of these top 10 hashtags! Create a plot of hashtag timelines for the most interesting and frequent hashtags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/got_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect hashtags from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "for tw in text:\n",
    "    # Convert all hashtags to lowercase\n",
    "    hashtags.append([i[1:].lower() for i in tw.split() if i.startswith(\"#\")])\n",
    "hashtags = [w for sublist in hashtags for w in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = ['#' + ''.join(c for c in h if c not in string.punctuation) for h in hashtags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the frequency of different hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count = {}\n",
    "for w in hashtags:\n",
    "    if w in hashtag_count.keys():\n",
    "        hashtag_count[w] += 1\n",
    "    else:\n",
    "        hashtag_count[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_counts_df = pd.DataFrame({'words': list(hashtag_count.keys()),\n",
    "                                  'counts': list(hashtag_count.values())}).sort_values(by=['counts'],\n",
    "                                                                                       ascending=False,\n",
    "                                                                                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 2\n",
    "nrows = 1\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*10, nrows*8))\n",
    "\n",
    "# Number of hashtags plotted\n",
    "n_hash = 10\n",
    "\n",
    "# Choose colors for bars\n",
    "start = 0.0\n",
    "stop = 0.8\n",
    "cm_subsection = np.linspace(start, stop, n_hash) \n",
    "colors = [cm.Reds_r(i) for i in cm_subsection]\n",
    "\n",
    "axes[0].set_title('Top {0} hashtags during\\nthe #GoT #RedWedding episode'.format(n_hash),\n",
    "                  fontsize=titlesize, fontweight='bold')\n",
    "axes[1].set_title('Top {0} hashtags during\\nthe #GoT #RedWedding episode (log scale)'.format(n_hash),\n",
    "                  fontsize=titlesize, fontweight='bold')\n",
    "\n",
    "\n",
    "for i in range(ncols):\n",
    "    ax = axes[i]\n",
    "    ax.bar(x=hashtag_counts_df['words'][:n_hash],\n",
    "           height=hashtag_counts_df['counts'][:n_hash],\n",
    "           tick_label=hashtag_counts_df['words'][:n_hash],\n",
    "           color=colors\n",
    "          )\n",
    "    ax.set_xlabel('Hashtags', fontsize=axislabelsize)\n",
    "    ax.set_ylabel('Count', fontsize=axislabelsize)\n",
    "\n",
    "    ax.set_xticklabels(hashtag_counts_df['words'][:n_hash],\n",
    "                       rotation=60, ha='right', rotation_mode='anchor')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text of the tweets, and gather the 'real' words for each tweet.\n",
    "\n",
    "By 'real' words, there should be:\n",
    "* no punctuations\n",
    "* hashtags only without `#` mark\n",
    "* no user mentions\n",
    "* no URLs\n",
    "* no emojis\n",
    "* no numbers\n",
    "\n",
    "Count word occurrences, make a histogram of the occurrences. What are the top words? Are they what you expected?\n",
    "\n",
    "What crazy words did you get? Explain possible approaches, with which you could throw out this kind of junk text as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-filtering\n",
    "\n",
    "Filtering out user mentions, URLs, `#` from hashtags and emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_flt = text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating emoji pattern to remove them\n",
    "# Helped by https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                               \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_pre = ['@', 'http', 'RT']\n",
    "for i, tw in enumerate(text_flt):\n",
    "    for f in filters_pre:\n",
    "        if f in tw:\n",
    "            tw_tmp = filter(lambda x:f not in x, tw.split())\n",
    "            tw = \" \".join(filter(lambda x:f not in x, tw.split()))\n",
    "\n",
    "    if '#' in tw:\n",
    "        tw = tw.replace('#', '')\n",
    "\n",
    "    # Removing emojis\n",
    "    # Helped by https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "    text_flt[i] = emoji_pattern.sub(r'', tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second filtering\n",
    "\n",
    "Remove punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = string.digits + string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the top N words from the vocabulary\n",
    "top_k = None\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_flt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "text_seqs = tokenizer.texts_to_sequences(text_flt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the legngth of the longest vector\n",
    "max_length = calc_max_length(text_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_counts = dict(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df = pd.DataFrame({'words': list(word_counts.keys()),\n",
    "                               'counts': list(word_counts.values())}).sort_values(by=['counts'],\n",
    "                                                                                  ascending=False,\n",
    "                                                                                  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 1\n",
    "nrows = 1\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*10, nrows*8))\n",
    "\n",
    "# Number of words plotted\n",
    "n_words = 10\n",
    "\n",
    "# Choose colors for bars\n",
    "start = 0.0\n",
    "stop = 0.8\n",
    "cm_subsection = np.linspace(start, stop, n_hash) \n",
    "colors = [cm.Reds_r(i) for i in cm_subsection]\n",
    "\n",
    "axes.bar(x=word_counts_df['words'][:n_words],\n",
    "         height=word_counts_df['counts'][:n_words],\n",
    "         tick_label=word_counts_df['words'][:n_words],\n",
    "         color=colors\n",
    "        )\n",
    "\n",
    "axes.set_title('Top {0} used words in tweets during\\nthe #GoT #RedWedding episode'.format(n_words),\n",
    "               fontsize=titlesize, fontweight='bold')\n",
    "\n",
    "axes.set_xlabel('Words', fontsize=axislabelsize)\n",
    "axes.set_ylabel('Count', fontsize=axislabelsize)\n",
    "\n",
    "axes.set_xticklabels(word_counts_df['words'][:n_words],\n",
    "                     rotation=60, ha='right', rotation_mode='anchor')\n",
    "axes.tick_params(axis='both', which='major', labelsize=axisticksize)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "\n",
    "Extract the stopword list for the English language with the help of `nltk`. Download the standard Brown Corpus also from `nltk`, count the relative frequency of stopwords in both the Brown Corpus and the GoT tweets. Make a scatterplot of your results, try to explain possible similarities and deviations. What is the correlation in the stopword frequencies of the two datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "A really common tool to visualize texts is a wordcloud. Find a suitable library and create a meaningful wordcloud of the GoT tweets (e.g. leave out punctuation, stopwords etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tutorial at here: https://www.datacamp.com/community/tutorials/wordcloud-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(wordcloud.STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the wordcloud, I've used the most frequent words with the small trickery by creating a completely arbitrary corpus by filtering all other words from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect the N most common words from the text\n",
    "n = 2000\n",
    "\n",
    "text_WordCloud = ''\n",
    "for tw in text_flt:\n",
    "    text_WordCloud += ' '.join((w for w in tw.split() if(w in word_counts_df['words'][:n].values)\n",
    "                                                         and\n",
    "                                                         w not in stopwords)) + ' '\n",
    "\n",
    "print (\"There are {} words in the combination of all filtered tweets.\".format(len(text_WordCloud)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "got_WordCloud = wordcloud.WordCloud(width=1800, height=900,\n",
    "                                    max_words=60, background_color='black').generate_from_text(text_WordCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,15))\n",
    "axes.set_aspect('equal')\n",
    "axes.axis('off')\n",
    "\n",
    "axes.imshow(got_WordCloud, interpolation='bilinear')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Define a time window in which all tweets count as one document. Create the term-document matrix of the tweets for this time segmentation. Apply stemming and stopword filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Apply a TF-IDF weighting scheme for the term-document matrix by hand (e.g. do not use a built-in vectorizer, but normalize by text length with a summation etc. `numpy` or `pandas` is strongly suggested). Then, choose a topic detection method such as LSI or LDA, and run it on your matrix. Try to interpret your results! Are your topics meaningful? Which topics are the most representative of your document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Write an own name parser for the tweets, and consider all names that you find in the dataset as a node of a graph. Add 1 to the weight of an edge if two names occur in the same tweet. With the help of networkx, draw the weighted network of names from the text. Try to find a simple clustering algorithm in networkx, cluster the names in the dataset. Print or visualize your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "This episode caused severe disappointments in many viewers, because of the sudden death of too many of the favourite characters. Search for some sentiment analysis method, and create a timeline of sentiments based on the tweet texts. Do the sentiments on Twitter reflect the time of the worst scene?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
